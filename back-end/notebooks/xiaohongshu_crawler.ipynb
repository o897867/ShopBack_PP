{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# 小红书爬虫 - 关键字搜索与封面下载\n",
    "\n",
    "**功能说明:**\n",
    "- 根据关键字搜索小红书笔记\n",
    "- 提取笔记标题、链接、封面图片\n",
    "- 批量下载封面thumbnail\n",
    "- 导出数据为CSV格式\n",
    "\n",
    "**使用方法:**\n",
    "1. 安装依赖: `pip install requests pandas fake-useragent pillow`\n",
    "2. 设置搜索关键字\n",
    "3. 运行爬虫获取数据\n",
    "4. 下载封面图片到本地\n",
    "\n",
    "**注意:** 仅供学习研究使用，请遵守网站服务条款"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "from datetime import datetime\n",
    "\n",
    "# 图片处理\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# User-Agent伪装\n",
    "try:\n",
    "    from fake_useragent import UserAgent\n",
    "    ua = UserAgent()\n",
    "except ImportError:\n",
    "    print(\"Warning: fake_useragent not installed, using default UA\")\n",
    "    ua = None\n",
    "\n",
    "print(\"✓ 库导入成功\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "class CrawlerConfig:\n",
    "    \"\"\"爬虫配置\"\"\"\n",
    "    # 搜索关键字\n",
    "    KEYWORD = \"美食\"  # 修改为你想搜索的关键字\n",
    "    \n",
    "    # 获取数量\n",
    "    MAX_ITEMS = 20  # 最多获取多少条笔记\n",
    "    \n",
    "    # 请求延迟（秒）\n",
    "    REQUEST_DELAY = 1  # 每次请求间隔，避免频繁访问\n",
    "    \n",
    "    # 保存路径\n",
    "    OUTPUT_DIR = \"xiaohongshu_data\"  # 数据保存目录\n",
    "    IMAGE_DIR = \"xiaohongshu_images\"  # 图片保存目录\n",
    "    \n",
    "    # 请求头\n",
    "    HEADERS = {\n",
    "        'User-Agent': ua.random if ua else 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json, text/plain, */*',\n",
    "        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',\n",
    "        'Referer': 'https://www.xiaohongshu.com/',\n",
    "    }\n",
    "\n",
    "# 创建输出目录\n",
    "os.makedirs(CrawlerConfig.OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(CrawlerConfig.IMAGE_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"配置完成:\")\n",
    "print(f\"  关键字: {CrawlerConfig.KEYWORD}\")\n",
    "print(f\"  数量: {CrawlerConfig.MAX_ITEMS}\")\n",
    "print(f\"  保存路径: {CrawlerConfig.OUTPUT_DIR}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crawler-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XiaohongshuCrawler:\n",
    "    \"\"\"小红书爬虫类\"\"\"\n",
    "    \n",
    "    def __init__(self, config: CrawlerConfig):\n",
    "        self.config = config\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update(config.HEADERS)\n",
    "        self.results = []\n",
    "    \n",
    "    def search_notes(self, keyword: str, max_items: int = 20) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        搜索小红书笔记\n",
    "        \n",
    "        注意: 小红书有严格的反爬虫机制，直接API请求可能失败\n",
    "        这里提供两种方案:\n",
    "        1. 模拟API请求（可能需要登录cookie）\n",
    "        2. 使用selenium/playwright模拟浏览器（更稳定但较慢）\n",
    "        \"\"\"\n",
    "        print(f\"开始搜索关键字: {keyword}\")\n",
    "        \n",
    "        # 方案1: 尝试直接API请求 (可能需要更新API地址和参数)\n",
    "        # 小红书的搜索API经常变化，这里提供基础模板\n",
    "        search_url = \"https://edith.xiaohongshu.com/api/sns/web/v1/search/notes\"\n",
    "        \n",
    "        params = {\n",
    "            'keyword': keyword,\n",
    "            'page': 1,\n",
    "            'page_size': max_items,\n",
    "            'search_id': int(time.time() * 1000),\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(search_url, params=params, timeout=10)\n",
    "            print(f\"状态码: {response.status_code}\")\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                return self._parse_search_results(data)\n",
    "            else:\n",
    "                print(f\"请求失败: {response.status_code}\")\n",
    "                print(\"建议: 使用selenium方案或添加登录cookie\")\n",
    "                return self._demo_data()  # 返回演示数据\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"请求异常: {e}\")\n",
    "            print(\"返回演示数据用于测试...\")\n",
    "            return self._demo_data()\n",
    "    \n",
    "    def _parse_search_results(self, data: dict) -> List[Dict]:\n",
    "        \"\"\"解析搜索结果\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        try:\n",
    "            items = data.get('data', {}).get('items', [])\n",
    "            \n",
    "            for item in items[:self.config.MAX_ITEMS]:\n",
    "                note = item.get('note_card', {})\n",
    "                \n",
    "                result = {\n",
    "                    'note_id': note.get('note_id', ''),\n",
    "                    'title': note.get('display_title', ''),\n",
    "                    'desc': note.get('desc', ''),\n",
    "                    'type': note.get('type', ''),\n",
    "                    'cover_url': note.get('cover', {}).get('url', ''),\n",
    "                    'user_name': note.get('user', {}).get('nickname', ''),\n",
    "                    'user_id': note.get('user', {}).get('user_id', ''),\n",
    "                    'liked_count': note.get('interact_info', {}).get('liked_count', 0),\n",
    "                    'note_url': f\"https://www.xiaohongshu.com/explore/{note.get('note_id', '')}\",\n",
    "                }\n",
    "                results.append(result)\n",
    "            \n",
    "            print(f\"成功解析 {len(results)} 条笔记\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"解析失败: {e}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _demo_data(self) -> List[Dict]:\n",
    "        \"\"\"返回演示数据（用于测试）\"\"\"\n",
    "        print(\"⚠️  使用演示数据（API请求失败）\")\n",
    "        return [\n",
    "            {\n",
    "                'note_id': 'demo001',\n",
    "                'title': '演示笔记1 - 美食分享',\n",
    "                'desc': '这是一个演示数据，实际使用需要配置cookie或使用selenium',\n",
    "                'type': 'normal',\n",
    "                'cover_url': 'https://picsum.photos/400/600?random=1',\n",
    "                'user_name': '演示用户1',\n",
    "                'user_id': 'demo_user_1',\n",
    "                'liked_count': 1234,\n",
    "                'note_url': 'https://www.xiaohongshu.com/explore/demo001',\n",
    "            },\n",
    "            {\n",
    "                'note_id': 'demo002',\n",
    "                'title': '演示笔记2 - 旅行攻略',\n",
    "                'desc': '这是一个演示数据，实际使用需要配置cookie或使用selenium',\n",
    "                'type': 'video',\n",
    "                'cover_url': 'https://picsum.photos/400/600?random=2',\n",
    "                'user_name': '演示用户2',\n",
    "                'user_id': 'demo_user_2',\n",
    "                'liked_count': 5678,\n",
    "                'note_url': 'https://www.xiaohongshu.com/explore/demo002',\n",
    "            },\n",
    "        ]\n",
    "    \n",
    "    def download_image(self, url: str, save_path: str) -> bool:\n",
    "        \"\"\"下载图片\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(url, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                # 保存图片\n",
    "                with open(save_path, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"下载失败: {url} (状态码: {response.status_code})\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            print(f\"下载异常: {url} ({e})\")\n",
    "            return False\n",
    "    \n",
    "    def download_thumbnails(self, results: List[Dict]) -> int:\n",
    "        \"\"\"批量下载封面\"\"\"\n",
    "        print(f\"\\n开始下载封面图片...\")\n",
    "        success_count = 0\n",
    "        \n",
    "        for i, item in enumerate(results, 1):\n",
    "            cover_url = item.get('cover_url', '')\n",
    "            if not cover_url:\n",
    "                print(f\"[{i}/{len(results)}] 跳过: 无封面URL\")\n",
    "                continue\n",
    "            \n",
    "            # 生成文件名\n",
    "            note_id = item.get('note_id', f'unknown_{i}')\n",
    "            ext = '.jpg'  # 默认jpg，也可以从URL解析\n",
    "            save_path = os.path.join(self.config.IMAGE_DIR, f\"{note_id}{ext}\")\n",
    "            \n",
    "            # 如果已存在则跳过\n",
    "            if os.path.exists(save_path):\n",
    "                print(f\"[{i}/{len(results)}] 跳过: {note_id} (已存在)\")\n",
    "                success_count += 1\n",
    "                continue\n",
    "            \n",
    "            # 下载\n",
    "            print(f\"[{i}/{len(results)}] 下载: {note_id}...\", end=' ')\n",
    "            if self.download_image(cover_url, save_path):\n",
    "                print(\"✓\")\n",
    "                success_count += 1\n",
    "                item['local_image_path'] = save_path\n",
    "            else:\n",
    "                print(\"✗\")\n",
    "            \n",
    "            # 延迟\n",
    "            time.sleep(self.config.REQUEST_DELAY)\n",
    "        \n",
    "        print(f\"\\n下载完成: {success_count}/{len(results)}\")\n",
    "        return success_count\n",
    "    \n",
    "    def save_to_csv(self, results: List[Dict], filename: str = None) -> str:\n",
    "        \"\"\"保存为CSV\"\"\"\n",
    "        if not filename:\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            filename = f\"xiaohongshu_{self.config.KEYWORD}_{timestamp}.csv\"\n",
    "        \n",
    "        filepath = os.path.join(self.config.OUTPUT_DIR, filename)\n",
    "        \n",
    "        df = pd.DataFrame(results)\n",
    "        df.to_csv(filepath, index=False, encoding='utf-8-sig')\n",
    "        \n",
    "        print(f\"\\n数据已保存: {filepath}\")\n",
    "        return filepath\n",
    "\n",
    "print(\"✓ 爬虫类定义完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-crawler",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行爬虫\n",
    "crawler = XiaohongshuCrawler(CrawlerConfig)\n",
    "\n",
    "# 搜索笔记\n",
    "results = crawler.search_notes(\n",
    "    keyword=CrawlerConfig.KEYWORD,\n",
    "    max_items=CrawlerConfig.MAX_ITEMS\n",
    ")\n",
    "\n",
    "# 显示结果预览\n",
    "if results:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"搜索结果预览 (共 {len(results)} 条):\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    for i, item in enumerate(results[:5], 1):  # 只显示前5条\n",
    "        print(f\"{i}. {item['title']}\")\n",
    "        print(f\"   作者: {item['user_name']}\")\n",
    "        print(f\"   点赞: {item['liked_count']}\")\n",
    "        print(f\"   链接: {item['note_url']}\")\n",
    "        print(f\"   封面: {item['cover_url'][:60]}...\" if len(item['cover_url']) > 60 else f\"   封面: {item['cover_url']}\")\n",
    "        print()\n",
    "    \n",
    "    if len(results) > 5:\n",
    "        print(f\"... 还有 {len(results) - 5} 条结果\\n\")\n",
    "else:\n",
    "    print(\"⚠️  未获取到数据\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "download-images",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载封面图片\n",
    "if results:\n",
    "    success = crawler.download_thumbnails(results)\n",
    "    print(f\"\\n图片保存目录: {CrawlerConfig.IMAGE_DIR}/\")\n",
    "else:\n",
    "    print(\"没有可下载的图片\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存数据为CSV\n",
    "if results:\n",
    "    csv_path = crawler.save_to_csv(results)\n",
    "    \n",
    "    # 显示DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    print(\"\\nDataFrame预览:\")\n",
    "    display(df[['title', 'user_name', 'liked_count', 'type']].head(10))\n",
    "else:\n",
    "    print(\"没有可保存的数据\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 进阶方案: 使用Selenium/Playwright\n",
    "\n",
    "如果上述API请求失败，可以使用浏览器自动化工具:\n",
    "\n",
    "```python\n",
    "# 安装: pip install playwright\n",
    "# 初始化: playwright install\n",
    "\n",
    "from playwright.sync_api import sync_playwright\n",
    "\n",
    "def search_with_browser(keyword: str):\n",
    "    with sync_playwright() as p:\n",
    "        browser = p.chromium.launch(headless=True)\n",
    "        page = browser.new_page()\n",
    "        \n",
    "        # 访问搜索页面\n",
    "        page.goto(f'https://www.xiaohongshu.com/search_result?keyword={keyword}')\n",
    "        page.wait_for_timeout(3000)\n",
    "        \n",
    "        # 提取数据\n",
    "        notes = page.query_selector_all('.note-item')  # 需要根据实际HTML调整\n",
    "        \n",
    "        results = []\n",
    "        for note in notes:\n",
    "            title = note.query_selector('.title').inner_text()\n",
    "            cover = note.query_selector('img').get_attribute('src')\n",
    "            results.append({'title': title, 'cover_url': cover})\n",
    "        \n",
    "        browser.close()\n",
    "        return results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "notes",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 使用说明\n",
    "\n",
    "### 1. 安装依赖\n",
    "```bash\n",
    "pip install requests pandas fake-useragent pillow\n",
    "# 可选: pip install playwright selenium\n",
    "```\n",
    "\n",
    "### 2. 修改配置\n",
    "在 `config` cell中修改:\n",
    "- `KEYWORD`: 搜索关键字\n",
    "- `MAX_ITEMS`: 获取数量\n",
    "- `REQUEST_DELAY`: 请求延迟\n",
    "\n",
    "### 3. 运行顺序\n",
    "1. 导入库 → 2. 配置 → 3. 定义爬虫类 → 4. 运行搜索 → 5. 下载图片 → 6. 保存数据\n",
    "\n",
    "### 4. 反爬虫处理\n",
    "- **添加Cookie**: 在浏览器登录后复制cookie到 `HEADERS`\n",
    "- **使用代理**: 配置代理IP池\n",
    "- **降低频率**: 增加 `REQUEST_DELAY`\n",
    "- **浏览器方案**: 使用playwright/selenium\n",
    "\n",
    "### 5. 输出文件\n",
    "- CSV数据: `xiaohongshu_data/`\n",
    "- 封面图片: `xiaohongshu_images/`\n",
    "\n",
    "---\n",
    "\n",
    "**免责声明**: 本代码仅供学习研究使用，请遵守小红书服务条款和robots.txt规则，不得用于商业用途或恶意爬取。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
